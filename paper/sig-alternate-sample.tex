% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{hyperref}

\begin{document}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{ }

% ISBN
\isbn{ }


%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{UofM:CoE:EECS}{2015, Ann Arbor MI}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{B-Trees for Insert Heavy Workloads}
%\subtitle{An in-depth analysis}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Isaac Bowen\\
       \affaddr{University of Michigan}
		\affaddr{College of Engineering}
		\affaddr{MSE: Computer Science and Engineering}
       \email{irbowen@umich.edu}
% 2nd. author
\alignauthor
Ryan Wawrzaszek\\
       \affaddr{University of Michigan}
		\affaddr{College of Engineering}
		\affaddr{MSE: Computer Science and Engineering}
       \email{ryanwawr@umich.edu}
}

\date{11 December 2015}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
\href{http://github.com/irbowen/b_trees}{Code can be found here}\\
Fill this in once we're done\\ \\
You should try to write the best research paper that you can using the results of your project. You have read many good papers throughout this class, so by this point you should have a good idea of what makes a good research paper! Basically, your report needs to clearly present the following:
\begin{enumerate}
\item the problem statement
\item The motivation, why its important
\item The literature review (the previous work in this area
\item Main idea and approach
\item Implementation techniques
\item Experimental setup
\item Results
\end{enumerate}
\end{abstract}

%
%  Use this command to print the description
%
%\printccsdesc

% We no longer use \terms command
%\terms{Theory}

%\keywords{ACM proceedings; \LaTeX; text tagging}
\keywords{B-Trees, Reader Writer Locks, B-Link}

\section{Introduction/Motivation}
Most traditional database management systems (DBMSs) use structures called B-Trees for storing indexes. B-Trees were designed to allow for efficient insertion and lookup of key-value pairs. However, this structure was developed decades ago, when DBMSs were run on single machines with small amounts of main memory, distributed applications were rare, and high availability was not a major concern. These circumstances influenced the design and structure of the B-Tree in critical ways, but database technology is a rapidly changing field. We have seen huge increases in processing power and main memory sizes, distributed applications have become commonplace, and clients have come to expect "always-on" systems with high availability for concurrent access. All of this means that many of the needs and assumptions that motivated the design of the B-Tree no longer hold true. 

Our particular focus in this paper is on the performance of B-Trees in in-memory databases. In the early days of relational databases, main memory was often too small to hold the full index. Therefore, B-Trees were designed to have a large fanout, since each level of the tree that had to be traversed incurred an additional disk I/O. Multi-threading was introduced to B-Trees, in part, to allow the processor to continue doing useful work while the next node of the tree was read into memory. Modern databases have advanced to the point where entire databases, even those containing several terabytes of information, can be stored in main memory. Our aim for this project was to analyze several different B-Tree implementations under an insert-heavy workload similar to those of modern OLTP databases.

\section{Previous Work}
B-trees have long been the primary access data structure for  relational databases. They have many properties that make them ideal structures for storing, retrieving, and using large amounts of data.\\
These include:
\begin{enumerate}
\item Efficient operations - Since all data is stored in the leaf nodes of the tree, the \texttt{update()} and \texttt{get()} operations only need to access one node at each level of the tree. Therefore, their runtime is logarithmic with respect to the fanout. The \texttt{insert()} operation is slightly more complex, as splits occurring in leaf nodes may propagate back up the tree, but the runtime is still logarithmic.
\item Maintaining elements in sorted order - In addition to allowing for efficient operations, maintaining elements in a sorted order allows for efficient joins on clustered indexes. Since selected tuples are retrieved in order, the sort-merge-join (SMJ) algorithm can be used without the expensive preprocessing step, allowing for improved query response times.
\item Block access - B-Trees often set the size of each node in the tree to be equivalent to the size of a memory page, adjusting the fan out of the system correspondingly in order to maximize space utilization. This also allows for efficient locking, as one needs only to lock the block id in order to lock the associated page. This made B-Tree's very popular early on, when most databases could not fit in memory, and had to be stored on disk.
\item Granular locking - B-Tree nodes can be locked individually. On each access, the DBMS only needs to lock the nodes that might be affected by the current operation. So long as the locking scheme maintains consistency within the tree, multiple threads are able to access the tree at once, improving concurrency and throughput.
\end{enumerate}

Many different structures and modifications of B-Trees have been proposed over the last 40 years.  In the original version of this data structure, data was stored in the inner nodes as well as in the leaf inner nodes.  This was seen as an optimal solution at the time as there was no wasted disk bandwidth, an important factor in an era when many DBMS applications were still single-threaded. A structure that limited data to the leaves would have wasted disk bandwidth by reading in a full page of data for each inner node, even though only the list of child nodes was needed. 

As applications became multi-threaded, new versions of the structure, such as B*Trees or B+Trees, switched to storing data exclusively in leaf nodes, storing only keys in the inner nodes. This loss of some amount of disk bandwidth associated with reading non-leaf nodes into memory was less critical in a multi-threaded environment. This shift in organization also allowed more keys to be stored in inner nodes, which gave trees a wider fanout and led to less total disk I/Os. Later versions of B-Trees also guaranteed that the tree structure was height balanced in order to provide good worst-case performance. They also added pointers linking leaf nodes together so that tables scans could be accomplished with only one traversal of the tree.  

CITE THE PAPERS SOMEWHERE

Later implementations added "sibling pointers" to inner nodes as well in an attempt to prolong splitting of nodes, which was a more expensive operation that a simple insert or retrieval. When a key was inserted into a node that would ordinarily be forced to split, it would instead check if it's sibling nodes had room for an additional key. If the sibling nodes were not full, keys were redistributed between the nodes in an order-preserving fashion. This allowed for delayed splitting of nodes and resulted in nodes that were, on average, closer to capacity. This served to minimize the depth of the tree which, in turn, decreased the amount of total disk I/Os needed to insert or retrieve a record.

Once DBMS applications moved to multi-threading, it became necessary to modify B-Trees to support concurrent access.  Several different locking schemes have been used in traditional B-Trees. The naive approach is to lock any node that could split on an insert.  It is impossible to know ahead of time which nodes will be accessed and which can potentially split, since, in a tree where each node is at full capacity, a split could propagate all the way up the tree to the root node. The simplest approach, then, is to acquire an exclusive lock on the root node before inserting, thereby locking the entire tree\cite{graefe:survey}. This is, in effect, no different from a single-threaded application, as no concurrency can be supported by this locking mechanics.

A slightly more refined locking mechanism involves acquiring a shared lock on the root node at the beginning of the operation. As the operation traverses the tree, it checks each node to see if it may be split by this operation. If it is determined that the child node cannot split, the shared lock is maintained. Otherwise, an exclusive lock must be acquired on both the child and parent nodes. This allows an increased degree of concurrency, as read operations can still proceed in a large portion of the tree while the insert takes place\cite{graefe:survey}\cite{lehman:locking}.  It should be noted, however, that this locking scheme can still lead to contention for a lock on the root node, especially in trees with small fanouts where nodes are split with a higher frequency.

\section{Our Approach}
We began by implementing a sequential access B-Tree with no locking mechanism and no support for concurrent access. This tree served as our baseline for measuring the performance of the concurrent access trees that we will discuss in Section 4. The sequential tree, as well as all other implementations, support the \texttt{insert(key, value)} and \textbf{get(key)} operations. We felt that this set of operations was sufficient to test our B-Tree implementations and locking mechanisms, as they provide the read and write functionality that will be needed to test our granular locking schemes.

We designed each B-Tree implementation in a manner that allows the fanout to be adjusted by setting a parameter. We were particularly interested in effect of node fanout on the performance of our implementations, given that the index and database are hosted entirely in main memory. Tree traversal is expensive for databases that are too large to fit in main memory, as a disk I/O is incurred every time a node is read in. This cost is not nearly as severe for an in-memory database, so we hypothesized that we would see less of a decrease in performance as fanout increased than would be observed in older databases.

We used the \texttt{std::rand()} function in C++ to generate key-value pairs for insertion and to determine whether an operation would be an insert or a retrieval. The percentage of reads and writes in the workload can be adjusted by means of an input parameter given to the program at runtime. We tested our implementations on insert-heavy workloads in order to simulate the workload of a modern OLTP database.

Given that our B-Tree implementations are intended for use in an in-memory database subject to insert-heavy workloads, we made several changes to the traditional B-Tree structure.  First, we do not we do not require the tree to be height-balanced.  Balanced trees are essential in DBMS applications for which the index and data are not hosted in memory, as a disk I/O is incurred for every level of the tree that is traversed. Therefore, balancing was required to limit disk I/O and provide reliable response times. For an in-memory database, the cost of reading node is merely the cost of following a pointer, so operation latencies are unlikely to vary significantly in an unbalanced tree. 
???
This makes sense for reads - you want to make sure each key get be access in the same amount of time, and if you are trying to get multiple elements, you can do the lookup once and then just scan across the leaf nodes.  However, maintain this global depth is expensive in the face of multiple threads.  We chose to abandon this requirement, and let the tree have varying depths of nodes - the root node could point to some leaf nodes, and other trees with various children.  This could increase lookup time (as well as insert time, since it must do at lookup to find the insert location, however, as long as the key values are not very skewed, the tree should perform well.)  We believe that additional lookup time is not a problem, as long as it is concurrent - the real performance limitation is when the tree structure must change, because this requires exclusive locks.  If we can keep the "structure" of the tree the same, then we can maximize for inserts, and always have some number of threads doing something useful, instead of waiting as the tree is balanced.
???

Many B-Trees implement pointers to the neighboring nodes at the same level, both for leaf and inner nodes.  While this is certainly helpful for range-based read workloads, there is a cost that has to be paid on every insert.  In an insert-heavy workload, these become much more important.  Also, in the inner nodes, these pointers are used to push of the tree splitting, to lower cost.  This was assuming, however, that the tree was balanced.  Without this invariant, there is no reason to do this.  In concurrent code, managing all of these pointers become incredibly difficult to do correctly, and has minimal benefit.  Therefore, we decided to forgo these pointers.  This means that range queries become less efficient, because we can no longer scan across the leaf nodes.  However, for an insert heavy workload, range queries are not well defined.  They would require some form of predicate locking, MVCC, or some other complication that we do not want - simple put, workloads that are insert heavy should know that range based queries do not make sense, as the data is always being changed.
???

\section{B-Tree Implementations}
The goal of our experiment is to observe the performance impact of varying each of the following parameters 
\begin{itemize}
  \item Input Size
  \item Number of Threads
  \item Read Write Percent Workload
  \item Fan out on inner nodes
  \item Data slots on leaf nodes
\end{itemize}
In order to examine the performance of B-Trees in in-memory databases, we built four different tree implementations.
\subsection{Sequential Tree}
We had to build a simple, single-thread B-tree first.  We built on this for our other version, and will also use it as the baseline for testing.  There are three main classes used in the implmentation - Tree, Inner\_Node, and Leaf\_Node.  The tree class manages the root node, which is always an Inner\_Node.  Both Inner\_Node and Leaf\_Node inheirt from a Node, so they can call functions on their children without knowing what type of node they are.  Operations are generally executing by calling the function on the Tree class, which then calls the corresponding function on the root node.  The root node will then call the same function on the correct child, until the correct node has been found and the function can execute.
\subsection{ReaderWriter Trees (Array and List-based)}
This tree uses different kinds of locks, shared and exclusive, and also does checking to see if the child node can split, to see if this node is "safe" from splitting.  This implmentation is based on the idea's presented in \cite{lehman:locking}.
When an insert begins, the Tree class calls insert on the root node.  The root node first acquires a shared lock, and then scans through the node's keys, looking for what child it should insert the new (key, value) pair into.  Once it has found the node, it checks if the child node can split.  If the child can split, than the parent switches to an exclusive lock - if the child splits, a new key will need to be pushed upwards in the tree, which would modify the parents state.  This allows inserts to only used shared reader locks in the top levels of the tree, which allows more concurrency, and switch to exclusive locks only when aboslutely nessecary.  However, locking a single node with an exclusive locks prevents any threads from inserting into that node or any of its children until the first insert operation has completed.
When a lookup begins, the Tree class calls the lookup function on the root node. The root node acquires a shared lock, and then finds the correct child, just like in an insert. Once it finds the child, it simply calls the lookup function on that child and repeats the process.  This implmentation does allow multiple threads to read the tree at any one time, however, if any node has an exclusive lock, it prevents any threads from readings itself or any node below it.
We create two different versions of the tree using reader writer locks - one that uses arrays to store the keys and values, and the other that uses lists.  The code between the two versions is very similar, the only difference being the data strucuture Inner\_Nodes use to store their children and Leaf\_Nodes use to store their values.
\subsection{B-Link Tree}
The B-Link tree is based on the work of CITE STUFF HERE and employs a bottom-up locking mechanism. The structure of the B-Link tree differs from that of a traditional tree in two main ways.

First, each node maintains a "link pointer" to the next node to the right at the same level. The pointer for the node furthest to the right is simply assigned a null value. These pointers appear as the horizontal arrows connecting nodes at the same level in Figure 1.

Second, each node maintains a "high key". The high key is the value of the highest key found in the subtree rooted at the node pointed to by the last non-link pointer in the node.

These structural modifications allow for the implementation of a minimal and efficient locking scheme. On an insertion, each node accessed as the tree is traversed is pushed onto a stack, but is not locked. The first node to be locked is the leaf into which the new value is being inserted. If the insertion causes the node to exceed its capacity, the node is split. A link pointer is added from the original node to the new node, and the link pointer of the new node is set equal to the old link pointer of the original node.

Because the leaf's parent node does not know of the newly created leaf node, the two leaves can be considered one leaf, as the new node is essentially an extension of the original that can be reached by following the link pointer. The parent node is popped from the stack and locked. A pointer to the newly formed node is inserted, atomically adding the new node to the tree. At this point, the leaf node may be unlocked. If the parent node is forced to split, the same split procedure is followed. This continues up the tree until the split cannot propagate any further, at which point the last lock is released.

The search algorithm for the B-link tree is very similar to that of a traditional B-tree, but it acquires no locks. As the search travels down the tree, it scans each node to find the appropriate child to insert into. If the high key of the current node is less than the key being inserted, then this node has been split, and the search simply follows link pointers until it finds the node that contains a pointer to the proper child node.
\section{Implementation and Test Setup}
We implemented our B-Trees in \texttt{C++}, using new features only available in \texttt{C++14}, most notably \texttt{stdshared\_time\_mutex}, which allows both exclusive and shared locking.  This requires a compiler with \texttt{gc -4.9} or greater.  Because this is so new, few systems have made version 4.9 available. We were unable to update the compiler on \texttt{bigdata.eecs.umich.edu}, so we set up our own testing environment.

We set up a virtual machine using Oracle Virtual Box.  The VM is running Ubunutu Server 15.10 with gcc 4.9.3.  The host system is a Windows 7 machine with a FX-8350 (\texttt{8 cores @ 4Ghz}) and 32GB of RAM.  We allocated 4 cores and 24GB of RAM to the VM.  All test were run with minimal applications running on the Windows side.

We used \texttt{std::rand()} to generate input data.  

\section{Performance}
We tested all of our implementations under several different conditions.

\subsection{Sequential}
We begin with our sequential tree implementation, so that we have a baseline to compare our concurrent versions to.  These are 100\% insert workloads.
\subsubsection{100\% Insert}
Sequential Tree, F = 4\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	2	\\
  10000			&	12	\\
  100000		&	80	\\
  1000000		&	586	\\
  10000000		&	5620	\\
  100000000		&	59000	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 16\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	2	\\
  10000			&	20	\\
  100000		&	123	\\
  1000000		&	754	\\
  10000000		&	7045	\\
  100000000		&	70318	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 64\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	4	\\
  10000			&	31	\\
  100000		&	283	\\
  1000000		&	3607	\\
  10000000		&	128447	\\
  100000000		&	384118\\
  \hline
\end{tabular} \\

Sequential Tree, F = 256\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	7	\\
  10000			&	67	\\
  100000		&	968	\\
  1000000		&	14840	\\
  10000000		&	267131	\\
  100000000		&	2543760	\\
  \hline
\end{tabular}\\

Note the dramatic difference in performance as the fan out increases. 
We discuss the significance of these results in the next section.\\

We wanted to compare are tree to standard implementations to a standard, to verify that our concurrent versions were not built on top of something slow.  To that end, we compared it to std::map on gcc 4.9.3.

\texttt{std::map}\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	1	\\
  10000			&	6	\\
  100000		&	63	\\
  1000000		&	720	\\
  10000000		&	12780	\\
  100000000		&	228835	\\
  \hline
\end{tabular}\\

\subsubsection{90\% Insert, 10\% Lookup}
While we are primary focused on insert performance, we wanted to see how a 90\% insert 10\% lookup workload would perform.\\
Sequential Tree, F = 4\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	1	\\
  10000			&	9	\\
  100000		&	58	\\
  1000000		&	527	\\
  10000000		&	5199	\\
  100000000		&	79855	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 16\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	2	\\
  10000			&	9	\\
  100000		&	112	\\
  1000000		&	667	\\
  10000000		&	6571	\\
  100000000		&	65188	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 64\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	11	\\
  10000			&	33	\\
  100000		&	283	\\
  1000000		&	4891	\\
  10000000		&	112838	\\
  100000000		&	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 256\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	9	\\
  10000			&	69	\\
  100000		&	986	\\
  1000000		&	14619	\\
  10000000		&	242493	\\
  100000000		&	*	\\
  \hline
\end{tabular}\\

\texttt{std::map}\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	1	\\
  10000			&	2	\\
  100000		&	34	\\
  1000000		&	716	\\
  10000000		&	13151	\\
  100000000		&	204166	\\
  \hline
\end{tabular}\\
We note that std::map seems to perform much better at reads.

\subsubsection{80\% Insert, 20\% Lookup}
While we are primary focused on insert performance, we wanted to see how a 80\% insert 20\% lookup workload would perform.\\
Sequential Tree, F = 4\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	1	\\
  10000			&	5	\\
  100000		&	60	\\
  1000000		&	469	\\
  10000000		&	4701	\\
  100000000		&	47418	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 16\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	2	\\
  10000			&	20	\\
  100000		&	92	\\
  1000000		&	606	\\
  10000000		&	6839	\\
  100000000		&	57951	\\
  \hline
\end{tabular} \\

Sequential Tree, F = 64\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	4	\\
  10000			&	32	\\
  100000		&	226	\\
  1000000		&	3216	\\
  10000000		&	82029	\\
  100000000		&	* \\
  \hline
\end{tabular} \\

Sequential Tree, F = 256\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	3	\\
  10000			&	50	\\
  100000		&	769	\\
  1000000		&	12465	\\
  10000000		&	182482	\\
  100000000		&	*	\\
  \hline
\end{tabular}\\

\texttt{std::map}\\
\begin{tabular}{| l | r |}
  \hline
  Input Size & Time (ms)\\  \hline
  1000			&	.5	\\
  10000			&	1	\\
  100000		&	23	\\
  1000000		&	584	\\
  10000000		&	10719	\\
  100000000		&	180852	\\
  \hline
\end{tabular}\\

\subsection{Concurrent, Single threaded}
We then testing our concurrent versions.
First we compared on its performance with one thread, and then we scale the number of threads to see how the performance changed.

\subsubsection{100\% Insert}
Reader Writer Tree List \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	13 & 105 & 1153 & 19787 & 301004 & * \\
  16		&	1 & 15 & 270 & 4694 & 79077 & * \\
  64		&	2 & 27 & 458 & 9124 & 156241 & * \\
  256		&	6 & 71 & 1409 & 25626 & 384319 & * \\
  \hline
\end{tabular} \\

Reader Writer Tree Array \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	10 & 78 & 861 & 14650 & 215321 & * \\
  16		&	2 & 21 & 117 & 1326 & 17772 & 246069 \\
  64		&	3 & 25 & 120 & 1191 & 14523 & 179811 \\
  256		&	11 & 59 & 412 & 4150 & 45198 & * \\
  \hline
\end{tabular} 


\subsubsection{90\% Insert, 10\% Lookup}
Reader Writer Tree List \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	12 & 106 & 1147 & 17987 & 287852 & * \\
  16		&	1 & 15 & 223 & 4223 & 75675 & * \\
  64		&	6 & 53 & 436 & 7965 & 138397 & * \\
  256		&	17 & 94 & 1317 & 22046 & 342301 & * \\
  \hline
\end{tabular} \\

Reader Writer Tree Array \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	9 & 79 & 870 & 13672 & 203731 & * \\
  16		&	1 & 10 & 109 & 1269 & 16874 & 235350 \\
  64		&	1 & 8 & 85 & 1113 & 12728 & 179811 \\
  256		&	3 & 37 & 347 & 3827 & 41877 & * \\
  \hline
\end{tabular} 

\subsubsection{80\% Insert, 20\% Lookup}
Reader Writer Tree List \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	10 & 76 & 1024 & 16335 & 266000 & * \\
  16		&	4 & 32 & 204 & 3894 & 65661 & * \\
  64		&	2 & 48 & 373 & 7321 & 122407 & * \\
  256		&	14 & 75 & 1117 & 20417 & 321325 & * \\
  \hline
\end{tabular} \\

Reader Writer Tree Array \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Input Size & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\ \hline
  Fan out & & & & & &  \\  \hline
  4			&	3 & 48 & 811 & 12197 & 191958 & * \\
  16		&	1 & 11 & 106 & 1182 & 16112 &  \\
  64		&	1 & 22 & 107 & 1048 & 12947 &  \\
  256		&	3 & 30 & 301 & 3426 & 37719 & * \\
  \hline
\end{tabular} 


\subsection{Concurrent, Scaling Threads}

Reader Writer Tree List - Input Size of 10000 \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Threads & Time \\ \hline
  1 & 105 \\
  2 & 332 \\
  4 & 709 \\
  8 & 703 \\
  16 & 856 \\
  \hline
\end{tabular} \\

Reader Writer Tree List - Input Size of 100000 \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Threads & Time \\ \hline
  1 & 1403 \\
  2 & 4049 \\
  4 & 8569 \\
  8 & 9629 \\
  16 & 10290 \\
  \hline
\end{tabular} \\

Reader Writer Tree Array - Input Size of 10000 \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Threads & Time \\ \hline
  1 & 18 \\
  2 & 58 \\
  4 & 153 \\
  8 & 198 \\
  16 & 288 \\
  \hline
\end{tabular} \\

Reader Writer Tree Array - Input Size of 100000 \\
\begin{tabular}{| l | c | c | c | c | c | c | r}
  \hline
  Threads & Time \\ \hline
  1 & 104 \\
  2 & 452 \\
  4 & 1122 \\
  8 & 2629 \\
  16 & 3788 \\
  \hline
\end{tabular} \\

\section{Discussion}
We began by testing out our sequential tree, on a variety of input sizes.  We wanted to make sure that this implementation, that we would be building on top of for our concurrent versions, was efficient.  We testing with various fan outs, and decided to test for 4, 16, 64, and 256.  We were surprised to see the performance degrade ad the fan out increases.  We assumed that a larger node size would allow faster look-ups and inserts, because the operation would have to traverse fewer levels of the tree.  However,it seems that there is a difficult balance between traversal within a node, and traversal in the tree across nodes.

We saw that the fan out of 4 gave us the best performance, so we wanted to compare that to an in-memory tree.  We chose to compare to \texttt{std::map}, which is typically implemented using Red-black trees.  When fan out is small, either 4 or 16, the performance of our B-Tree is better.  However, as the fan out increases, our sequential tree performance degrades to the point where it is slower than \texttt{std::map}.

It is curious that the fan out increases seems to slow things down

Our concurrent versions, across the board, are slower than the sequential version.  This is even true when we run our concurrent versions with just one thread - the overhead of acquire locks is a huge performance hit.  Each request to lock or unlock is a call to the operating system, which could result in busy-waiting.  In as we scale up the number of threads, even with the same number of total inserts, we see a considerably performance degradation.  This is primarily due to contention on the root node.    The reader writer lock implementation still has to acquire exclusive locks on the root node when one of the inserts could result in one of the roots children splitting.  While the system allows for more concurrency than the naive scheme, the contention on the root node, as well as all of the locking overhead, create a system that is slower than inserting using the sequential lock free version.  This seems to be a fundamental flaw of any top down approach - the root node will always be the bottleneck, and the primary limitation to performance.

\section{Takeaways}
We have a few notable takeaways from our implementation and experimentation:
\begin{itemize}
\item \textbf{Locking} Locking is expensive.  Even in our single threaded tests, our multithreaded solutions are considerably slower than our single threaded versions.  The code is nearly identical, and so most of the extra time comes from locking overhead.  Each call to acquire a lock, or even check if it is locked, requires a trap into the OS, which is very expensive.  We saw similar findings in \cite{stone:oltp} and \cite{stone:era}.

\item \textbf{Single Threaded} Much like Stonebraker has mentioned in multiple papers, the traditional concurrent multithreaded code with locks was built for a different era - when indexes were stored on disk, and it made since to make node's the size of a disk block.  Now, it is faster to run operations in a single threaded fashion.  Even when we use our concurrent code, it is faster to run 10,000 insert operations in one thread than 5000 insert operations in two threads.  This is much like the findings in \cite{stone:era}

\item \textbf{Smaller Node} Smaller nodes seem to be faster than larger nodes.  While this seems to go against what we would expect with modern cache architecture, it does make sense.  Larger nodes were designed for block access devices, so it made sense to have the nodes be the same size as the disk blocks.  However, if the structure fits in memory, than it is fast and cheap to traverse the nodes, so it's not as important to keep the tree's depth very small.

\item \textbf{Vectors vs List} Two of our implementations are identical, except for the data structure the nodes use to store their children.  We can see that the array based version beats the list based version handily on every large case, especially when fan out is 16-64.  Arrays of this size fit in caches easily, and searching them sequentially is very quick.  We believe the use of contiguous memory is essential to performance.

\item \textbf{B-Tree vs Red black Tree} We noticed that the our sequential tree was able outperform \texttt{std::map}, but as the read percentage increased, \texttt{std::map}'s performance got dramatically better.  We thing that this is because of the underlying structures of our two implementations.  While we use a unbalanced B-Tree with high fan out(compared to Red-Black Trees), which allows for faster inserts.  However, \texttt{std::map} must consistently re-balance, adding overhead.  In higher read based workloads, it gets an advantage for that cost - look ups can be done very quickly.

\end{itemize}

\section{Citations}
Some papers that we read to give ourselves background on the topics:
\begin{itemize}
  \item "Concurrent B-trees with Lock-free Techniques"\cite{sultana:lockfree} gives us the inspiration for the lock free B-tree.  They, however, didn't not have access to the large memory, built in C++14 atomics, and they were not focusing on writes - they were trying to build a general purpose lock free btree for NUMA computers.
  \item "A survey of B-tree locking techniques"\cite{graefe:survey}, much like the title says, presents an over of many of the different approaches that have been taken so far.  However, it reallys skims over, almost dismissing, lock free techniques.
  \item "Efficient Locking for Concurrent Operations on B-Trees" \cite{lehman:locking} This one gave us the idea of the \texttt{can\_split()} function, and gave background on reader writer locks and other, etc
  \item "Concurrent Cache-Oblivious B-Trees" \cite{bender:cache} I need to read this one again
  \item "A Concurrent Blink-Tree Algorithm Using a Cooperative Locking Protocl" \cite{lim:blink} Ryan is making this verion
  \item "A paper" A paper
  \item GET STONEBRAKERS PAPERS WHERE HE TEARS APART TRANSACTIONS LOCKING AND TRADITIONAL DBMS
\end{itemize}

\section{Experimental Results}
We will need some tables.  I have some data from the sequential tree

\section{Conclusions}
Conclusions
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
Acknowledgments

\bibliographystyle{plain}
\bibliography{mybib}

\end{document}
